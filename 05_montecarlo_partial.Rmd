---
author: "YOUR NAME HERE"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

## 4.2 Monte Carlo-based sensitivity analysis
(20 pnts)

In this lab, you will use the transfer function model from the previous module to conduct a Monte Carlo–based sensitivity analysis. Rather than testing individual parameter changes, you will run the model many times using randomly sampled parameter values within defined ranges. 
Because model parameters are typically estimated rather than directly measured, it is important to understand how sensitive model performance is to those values. Monte Carlo simulation allows us to systematically explore parameter uncertainty and evaluate how it affects simulated discharge.

The Monte Carlo code provided will automatically perform many model runs inside a loop, (you will not need to manually adjust parameter values between runs as you did in the previous lab).

For each run, the code will:

--Sample a new parameter set
--Run the model
--Evaluate model performance
--For each run, you will evaluate model performance using one or more *objective functions* (i.e., quantitative metrics that measure how well simulated discharge matches observation (e.g., NSE, KGE, etc.)). 

After completing all simulations, you will apply the GLUE method to identify which parameter values are associated with better-performing model runs.

So our objectives in this lab are to:

1. Set up and implement the Monte Carlo simulation framework.
2. Run the model for one year of the study period and perform a sensitivity analysis.
3. Compare how different measures of model fit influence which parameter sets are classified as behavioral.

Much of the code is provided. Your responsibility is to understand what it is doing and interpret the results.

### Setup
#### Import packages
- including the "progress" and "tictoc" packages.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, comment = TRUE, message = TRUE, error = TRUE)

# Write your package testing function
pkgTest <- function(x)
{
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies= TRUE)
  }
  library(x, character.only = TRUE)
}

# Make a vector of the packages you need
neededPackages <- c('tidyverse', 'tictoc', 'patchwork', 'progress', 'plotly') 

# For every package in the vector, apply your pkgTest function
for (package in neededPackages){pkgTest(package)}

#If you have questions about a specific library, try ?packagename in your console (e.g., ?progress)
```

#### Read data and prep data
This is the same PQ data we have worked with in previous modules.

```{r read_in_data}
rm(list = ls(all = TRUE)) # clear global environment

indata <- read_csv("P_Q_1996_2011.csv")
indata <- indata %>%
  mutate(Date = BLANK(Date)) %>% # bring the date colum in shape
  mutate(wtr_yr = if_else(month(Date) > 9, year(Date) + 1, year(Date))) # create a water year column

# choose the 2006 water year
PQ <- indata %>%
  filter(BLANK == BLANK)
```

#### Define variables
```{r define_inputs}
tau <- 1:nrow(PQ) # simple timestep counter
Pobs <- PQ$RainMelt_mm # observed precipitation
Qobs <- PQ$Discharge_mm # observed streamflow
```

### Parameter initialization
This next chunk has two purposes. The first is to set up the number of iterations for the Monte Carlo simulation. The entire model code is essentially wrapped into the main MC for loop. Each iteration of that loop is one full model realization: loss function, TF, convolution, model fit assessment (objective function). For each model run (each MC iteration), we will save the model parameters and respective objective functions (a.k.a performance metrics, e.g. NSE, KGE) in a dataframe. This will be the main source for the sensitivity analysis at the end. The model parameters are sampled in the main model chunk, this is just the preallocation of the dataframe.  

You will both run your own MC simulation, to set up the code, but will also receive a .Rdata file with 100,000 runs to give you more behavioral runs for the sensitivity analysis and uncertainty bounds. As a tip, while setting up the code, I would recommend setting the number of MC iterations to something low, for example 10 or maybe even 1. Once you have confirmed that your code works, crank up the number of iterations. Set it to 1000 to see how many 'behavioral' or test runs you get. After that, load the file with the provided runs.  

```{r define_parameters}
# nx sets the number of Monte Carlo runs
nx <- BLANK # number of runs

# set up the parameter matrix for the loss function, transfer function, and objective functions
param <- tibble(
  Run = vector(mode = "double", nx), # counter for model run
  b1 = vector(mode = "double", nx),
  b2 = vector(mode = "double", nx),
  b3 = vector(mode = "double", nx),
  a = vector(mode = "double", nx),
  b = vector(mode = "double", nx),
  nse = vector(mode = "double", nx),
  kge = vector(mode = "double", nx),
  rmse = vector(mode = "double", nx),
  mae = vector(mode = "double", nx),
  qsim = vector(mode = 'double', nx)
  
)
```

### MC Model run

This is the main model chunk. The tic() and toc() statements measure the execution time for the whole chunk. There is also a progress bar in the chunk that will run in the console and inform you about the progress of the MC simulation. The loss function parameters are set in the loss function, the transfer function parameters in the loss function code. For each loop iteration, we will store the parameter values and the simulated discharge in the "param" dataframe. So, if we ran the MC simulation 100 times, we would end up with 100 parameter combinations and simulated discharges.

```{r model_chunk1, echo=F, eval=F}
tic("MC Simulation")

# initialize progress bar
pb <- progress_bar$new(
  format = " simulating [:bar] :percent eta: :eta",
  total = nx,
  clear = FALSE,
  width = 60
)

#### MONTE CARLO SETUP (use ii as index and let it run for as many iterations as we have set nx)
for (ii in 1:BLANK) {
  pb$tick() # needed for progress bar

  # write run number into the parameter matrix. this is just a simple counter.
  param$Run[ii] <- ii


  ######## LOSS FUNCTION #########
  # feel free to ignore the next two lines of code. There can be parameter combinations in the loss function that lead to an execution error. The "while" statement prevents this.
  p_eff <- 10000 # artificially large value to make the first while loop TRUE
  while (sum(p_eff) < 0.2 * sum(Pobs) ||
    sum(p_eff) > sum(Pobs) || is.na(sum(p_eff))) {
    
    
    # define the loss function parameters
    # Here, you need to specify how to generate the values for the loss function parameters. Use runif() for the sampling. For each loop iteration, we want to create one value for each parameter (and save it in param)
    param$b1[ii] <- runif(1, 0, 0.2) # let b1 vary from 0 to 0.2
    param$b2[ii] <- runif(BLANK) # let b2 vary from 0 to 50
    param$b3[ii] <- runif(BLANK) # let b3 vary from 0 to 50


    # preallocate the p_eff vector (length of the timeseries)
    p_eff <- vector(mode = "double", length(tau))

    # set a starting value for s (you are pulling the value out of the parameter dataframe)
    s <- param$b3[ii]

    # loop with loss function
    for (i in 1:length(p_eff)) {
      s <- param$b1[ii] * Pobs[i] + (1 - 1 / param$b2[ii]) * s
      p_eff[i] <- Pobs[i] * s
    }


    
    ####### TRANSFER FUNCTION #######
    # The next two lines are not really necessary but are some initial quality control on the TF parameters
    gTF <- tau
    while (sum(gTF[1:183], na.rm = TRUE) < sum(gTF[184:length(gTF)], na.rm = TRUE)) {
      # MC randomization for TF parameters
      param$a[ii] <- runif(1, 0, 20) # set the alpha parameter to a random value between 0 and 20
      param$b[ii] <- BLANK # set the beta parameter to a random value between 0 and 100

      # set up the TF (again, remember that the TF function parameters are coming from the param df and need to be indexed properly)
      g <- (tau^(param$a[ii] - 1)) / ((param$b[ii]^param$a[ii] * gamma(param$a[ii]))) * exp(-tau / param$b[ii])

      # normalization
      gTF <- g / sum(g)
    } # while (sum(gTF[1:183], na.rm = TRUE) < sum(gTF[184:length(gTF)], na.rm = TRUE)) {

    # quick plot of gTF
    qplot(tau, gTF, geom = "line")



    ####### CONVOLUTION ########
    # preallocate qsim vector
    Qsim <- vector(mode = "double", length(p_eff))

    # start convolution (this code for the second convolution method was developed by a previous student of this class. It cuts execution time by more than 60%)
    m <- length(p_eff)
    for (k in 1:length(p_eff)) {
      Qsim[k:m] <- Qsim[k:m] + p_eff[k] * gTF[1:(m - k + 1)]
    }

    # Save Qsim (or q_all) as vector in the parameter dataframe (by creating a 'list')
    param$qsim[ii] <- BLANK(Qsim) # this saves a vector in a single df cell



    ######## MODEL EFFICIENCY AND OBJECTIVE FUNCTIONS ########
    # This is an important component! We will also save values from several objective functions in the param df.
    # Use Qsim and Qobs as the two variables (quicker than referencing a df)

    ### DOUBLE AND TRIPLE CHECK PARENTHESES ############
# You may need to look up each of these coefficients to fill in the blanks. Searches like 'what is nse sensitivity analysis equation' can help you here.
    
    # NSE
    param$nse[ii] <- 1 - ((sum((BLANK - BLANK)^2)) / sum((Qobs - mean(Qobs))^2))

    # KGE
    kge_r <- cor(BLANK, BLANK) # correlation between Qobs and Qsim (simple Pearson)
    kge_beta <- mean(BLANK) / mean(BLANK) # ratio of the means
    kge_gamma <- (sd(Qsim) / mean(Qsim)) / (sd(Qobs) / mean(Qobs)) # ratio of the CVs
    param$kge[ii] <- 1 - sqrt((kge_r - 1)^2 + (kge_beta - 1)^2 + (kge_gamma - 1)^2) # entire obj fct put together

    # RMSE
    param$rmse[ii] <- sqrt(sum((Qobs - Qsim)^2) / length(Qobs)) # root mean square error obj fct

    # MAE
    param$mae[ii] <- sum(abs(Qobs - Qsim)) / length(BLANK) # mean absolute error obj fct
  }
}
toc()

# sort parameter matrix in descending order of objective function (best first, worst last). use nse first.
param <- arrange(param, desc(nse))
```

This chunk does a few things: it takes the Q simulations that are stored in param qsim as a list and "unlists" them into a single vector (unlist(param$qsim)); then it takes that vector and writes it into a matrix with as many columns as nx; then it converts the matrix to a dataframe. You might see a less convoluted way to do this, feel free to play with the code. 

```{r output_pivot, echo=F, eval=F}
  
PQ_plot <- as.data.frame(matrix(unlist(param$qsim), ncol = nx, byrow = F)) %>% 
  mutate(Date = PQ$Date) %>% # add the date vector
  pivot_longer(-Date, names_to = "Simulation", values_to = "Value") # make long form
```


```{r model_plot, echo=F, eval=F}
# plot the simulated flows
pq_plot <- ggplot(PQ_plot, aes(x=Date, y=Value, color=Simulation)) +
  geom_line()

ggplotly(pq_plot)
```

**Q1 (2 pt) The Monte Carlo loop randomly samples new parameter values for each run. What distribution is being used to sample the parameters? What assumption does this choice make about our prior knowledge of reasonable parameter values? (1–3 sentences)**  
ANSWER: 



**Q2 (3pnt) Understanding the loop: Describe what happens during a single Monte Carlo iteration (one pass through the loop). List the main steps (~6) from parameter sampling to saving model performance.**
ANSWER:



### Save or load data

After setting up the MC simulation, we will now use a pre-created dataset (like the param matrix generated above). Running the MC simulation tens of thousands of times will take multiple hours. For that reason, we ran it for you and will now use it as an existing data set.

```{r data_saving}
# save.image(file = 'AllData.RData')

# load the MC data
load("AllData.RData")
```

Best run

Now that we have the dataframe with all parameter combinations and all efficiencies, we can plot the best simulation and compare it to the observed discharge

```{r}
# take the best run. Note that we sorted the param matrix after the simulations above by NSE, so now we can evaluate the first row in param df), unlist the simulated discharge, and store it in a dataframe
PQ$Qsim <- BLANK(param$qsim[1])

# make long form
PQ_long <- PQ %>%
  select(-wtr_yr, -RainMelt_mm) %>% # remove the water year and rainmelt columns
  pivot_longer(names_to = "key", values_to = "value", -Date)

# plot observed and best simulated discharge in the same figure against the date
ggplot(PQ_long, aes(x = BLANK, y = BLANK, color = key)) +
  theme_bw(base_size = 15) +
  geom_line() +
  labs(y = "Discharge (mm/day)", color = {
  })
```

**Q3. (3pt) Using the arrange() function (see code after simulation above ~ line 220), rearrange the imported simulation results by any other calculated performance metric (like KGE or MAE...) instead and plot the observed and simulated discharge in the chunk below. Does the objective function you chose provide a better or worse match than NSE to observed runoff overall? Is it better at capturing peak flow events? How about baseflow conditions?** 

```{r}

```

ANSWER: 



### Sensitivity analysis
We will use the GLUE methodology (Beven and Binley, 1992) to assess parameter sensitivity. In this lab, GLUE serves two purposes:
--To evaluate parameter sensitivity, and
--To create an envelope of behavioral model simulations.

GLUE is used here because it provides an intuitive and visual way to explore parameter uncertainty and model performance. However, it is not the only approach to sensitivity or uncertainty analysis, and it has known limitations. In your own modeling work, the most appropriate method will depend on your research question, data availability, and disciplinary context.
For this exercise, we will use NSE as the performance metric to classify test runs, but this can be used for any performance metric. 

To visualize the 'behavior' of each range of parameter values, you want to plot each parameter in its own box. Look up facet_wrap() for how to do this! You want the axis scaling to be "free". We'll generate and evaluate 3 different kinds of plots:
-- dotty plots
-- density plots
-- 2d density plots

Each type of plot displays the relationship between parameter values and model performance, but in different ways. The goal is to compare how clearly each plot type reveals parameter sensitivity and patterns across all of the test runs.

```{r}
# select columns and make long form data
param_long <- param %>%
  select(-Run, -kge, -rmse, -mae, -qsim) %>% # remove unnecessary columns. We only want the five parameters and nse
  pivot_longer(names_to = "key", values_to = "value", -nse) # make long form


# set nse cutoff for behavioral model simulations. use 0.7 as threshold
cutoff <- BLANK
param_long <- param_long %>%
  filter(nse > BLANK) # use filter() to only use runs with nse greater than the cutoff
```


```{r dotty_plots}
# dotty plots
ggplot(param_long, aes(x = value, y = nse)) + # x is parameter value, y is nse value
  geom_point() + # plot as points
  facet_wrap(vars(key), scales = "free") + # facets are the individual parameters
  ylim(cutoff, 1) + # sets obj fct axis limit from the cutoff value to 1
  theme(
    strip.text = element_text(face = "bold", size = 8),
    strip.background = element_rect(
      fill = "gray95",
      colour = "gray",
      size = 0.5
      )
  )
```


```{r density_plots}
# density plots
ggplot() +
  theme_bw(base_size = 15) +
  geom_density(data = param_long, aes(x = value)) + # geom_density() to plot the density
  BLANK(~key, scales = "free") + # facet_wrap() to get each parameter in its own box
  theme(
    strip.text = element_text(face = "bold", size = 8),
    strip.background = element_rect(
      fill = "gray95",
      colour = "gray",
      size = 0.5
      )
  )
```


```{r 2ddensity_plots}
# 2d density plots
ggplot() +
  BLANK( # geom_density_2d_filled() for the actual density plot
    data = param_long,
    aes(x = value, y = nse),
    alpha = 1,
    contour_var = "ndensity"
  ) +
  geom_point( # geom_point() to show the indivudal runs
    data = param_long,
    aes(x = value, y = nse),
    shape = 1,
    alpha = 0.2,
    size = 0.5,
    stroke = 0.2,
    color = "black"
  ) +
  theme_bw(base_size = 15) +
  facet_wrap(~key, scales = "free") +
  theme(
    strip.text = element_text(face = "bold", size = 8),
    strip.background = element_rect(
      fill = "gray95",
      colour = "gray",
      linewidth = 0.5
    )
  ) +
  labs(x = "Value", y = "NSE (-)") +
  theme(legend.title = element_blank(), legend.position = "none")
```

**Q4 (3 pts) Examine the dotty plots and density plots for each parameter. For each parameter, determine whether model performance (NSE) appears to improve within a specific range of values or if high NSE values occur across most of the parameter range. Based on your plots:     1. Identify at least one parameter that appears sensitive and explain what visual evidence supports your conclusion.                              2. Identify at least one parameter that appears relatively insensitive and explain why.**

Before answering the question below, revisit the script above or the transfer function lesson and remind yourself what physical processes or watershed characteristics each parameter represents. Sensitivity is not inherently “good” or “bad.” Instead, consider whether it makes sense for the model to be sensitive (or insensitive) to that parameter given what it represents. For example, you might have greater confidence in a model that is sensitive to parameters representing relatively stable or spatially uniform watershed characteristics, and less confidence if model performance depends heavily on parameters representing highly uncertain or poorly constrained processes.

**Q5 (3 pts) Briefly discuss how parameter sensitivity (or lack of it) influences your confidence in model predictions. Use terms that describe what the parameter represents rather than the variable we assigned it.**
   
ANSWER: 



### Uncertainty Bounds
Now that we have evaluated how different parameter combinations affect model performance, we can visualize the uncertainty in simulated discharge that results from parameter uncertainty.

Instead of focusing on a single “best” simulation, we will use only the test runs that met our NSE threshold above and examine the range of simulated discharge they produce.
The goal is to answer the question: 'Given the parameter combinations that perform reasonably well, how much variability exists in the simulated hydrograph?'

To do this, we will:
-- Filter the Monte Carlo results to keep only runs that exceed the chosen NSE threshold (as above).
-- Extract the simulated discharge time series from those runs.
-- For *each day* in the simulation period, calculate:
  ***The minimum simulated discharge
  ***The maximum simulated discharge
These daily minimum and maximum values define an uncertainty envelope around the simulations.
The shaded region in the final plot represents the range of plausible discharge values given our accepted parameter sets. The best-performing simulation is shown for reference, along with the observed discharge.

```{r}
# remove non-behavioral runs using the cutoff
param_ci <- param %>%
  filter(nse > 0.872) # only use obj function (nse) values above the previously defined threshold

# make df with all of the top runs. this saves each of the top simulated Q time series in its own column.
# the columns are called V1 through Vn, with V1 being the best simulation and Vn the worst simulation of the behavioral runs.
Qruns <-
  as_tibble(matrix(
    unlist(param_ci$qsim),
    ncol = nrow(param_ci),
    byrow = F
  ), .name_repair = NULL)

# combine Qruns with date and observed runoff from the PQ data frame. additionally, calculate mins and maxs
Qruns <-
  bind_cols(Date = PQ$Date, Qruns) %>% # bind_cols() to combine Date, RainMelt_mm, and Qruns
  mutate(Qmin = apply(Qruns, 1, BLANK)) %>% # get the row min for simulated Qs
  mutate(Qmax = apply(Qruns, 1, max)) # get the row max for simualted Qs

# long form
Qruns_long <- Qruns %>%
  # select(-Discharge_mm, -Qsim) %>% # remove observed and best simulated discharge
  pivot_longer(names_to = "key", values_to = "value", -Date) # make long form

# plot with all simulated runs
ggplot() +
  geom_line(data = BLANK, aes(x = Date, y = value, color = key)) + # plot of all simulated runs. Use Qruns_long here.
  guides(color=FALSE) +
  geom_line(
    data = Qruns,
    aes(x = Date, y = V1, color = "Best run"),
    size = 1
  ) + # plot of best simulation. Use Qruns here. V1 is the best simulated discharge
  labs(y = "Q (mm/day)", x = {
  }, color = {
  })
```


```{r}
# Lets add the best simulated Q and observed Q to the plot. You need Qruns for the simulated Q and envelope and PQ to plot the observed streamflow.
ggplot() +
  geom_ribbon(data = Qruns, aes(x = Date, ymin = Qmin, ymax = Qmax)) + # plot of envelopes. look up geom function that allows you to plot a shaded area between two lines
  # plot the best simulated run. remember, that is V1 in the Qruns df
  geom_line(
    data = Qruns,
    aes(x = Date, y = BLANK, color = "Best run"),
    size = 0.6
  ) + # plot of best simulation
  geom_line(
    data = PQ,
    aes(x = Date, y = Discharge_mm, color = "Observed Q"),
    size = 0.6
  ) + # plot of observed Q
  labs(y = "Q (mm/day)", x = {
  }, color = {
  })
```

**Q6 (3 pts) Describe what the uncertainty envelope represents in this context. How was it constructed? Is it appropriate to interpret this envelope as a statistical confidence interval or prediction interval? Why or why not? (2–4 sentences)**   
ANSWER: 



**Q7 (3 pts) If you inspect the individual model runs (in the Qruns df), you will notice that they all perform somewhat poorly when it comes to the initial baseflow. Consider how the model is initialized at the beginning of the time series. Why might the early simulation period be less reliable than later periods? What modification to the modeling approach could help address this issue? (Note: you don't have to actually do this, just describe how you might approach that issue. (2-3 sentences)** 

ANSWER: 

